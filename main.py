import streamlit as st
import os
from langchain.memory import ConversationBufferMemory
from utils import qa_agent

st.set_page_config(
    page_title="èŠ¯æ™ºé€š-åŠå¯¼ä½“çŸ¥è¯†åº“",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stChatInput {padding: 20px;}
    .st-expander {border: 1px solid #4a4a4a;}
    .reportview-container .main .block-container {padding-top: 2rem;}
    .sidebar .sidebar-content {background: #f0f2f6;}
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»Ÿé…ç½®")

    # è‡ªåŠ¨è·å–ç¯å¢ƒå˜é‡
    env_key = os.getenv("COURSE_API_KEY")
    api_base = os.getenv("OPENAI_API_BASE", "https://api.aigc369.com/v1")

    # åŠ¨æ€æ˜¾ç¤ºå¯†é’¥è¾“å…¥
    if not env_key:
        user_key = st.text_input(
            "OpenAI APIå¯†é’¥ï¼š",
            type="password",
            help="æœªé…ç½®ç¯å¢ƒå˜é‡æ—¶éœ€æ‰‹åŠ¨è¾“å…¥"
        )
    else:
        st.success("âœ… å·²æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡å¯†é’¥")
        user_key = ""

    # æ¨¡å‹é€‰æ‹©
    ai_model = st.selectbox(
        "é€‰æ‹©AIæ¨¡å‹",
        ["gpt-3.5-turbo", "gpt-4"],
        index=0,
        help="æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"
    )

    st.divider()
    st.markdown("### æ”¯æŒæ–‡æ¡£æ ¼å¼")
    st.markdown("""
    - å·¥è‰ºæ‰‹å†Œ (PDF/DOCX)
    - è®¾å¤‡å‚æ•°è¡¨ (CSV/TXT) 
    - ç ”å‘æ–‡æ¡£ (Markdown)
    - ä¸“åˆ©æ–‡ä»¶ (PDF)
    """)

# ä¸»ç•Œé¢
st.title("ğŸ”¬ èŠ¯æ™ºé€š - åŠå¯¼ä½“çŸ¥è¯†åº“ç³»ç»Ÿ")
st.caption("æ”¯æŒèŠ¯ç‰‡åˆ¶é€ å…¨æµç¨‹æ–‡æ¡£åˆ†æï¼šå…‰åˆ»/åˆ»èš€/æ²‰ç§¯/å°è£…å·¥è‰º")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader(
    "ä¸Šä¼ æŠ€æœ¯æ–‡æ¡£",
    type=["pdf", "docx", "txt", "csv", "md"],
    help="æœ€å¤§æ–‡ä»¶å¤§å°ï¼š50MB"
)

# å¯¹è¯ç•Œé¢
if uploaded_file:
    # è¾“å…¥åŒºåŸŸ
    col1, col2 = st.columns([6, 1])
    with col1:
        question = st.text_input("æŠ€æœ¯é—®é¢˜", placeholder="è¯·è¾“å…¥å…³äºèŠ¯ç‰‡åˆ¶é€ çš„æŠ€æœ¯é—®é¢˜...")
    with col2:
        submit_btn = st.button("å‘é€", use_container_width=True)

    # å¤„ç†é—®ç­”
    if submit_btn and question:
        final_key = user_key or env_key
        if not final_key:
            st.error("éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥")
            st.stop()

        with st.spinner("ğŸ” æ­£åœ¨è§£ææŠ€æœ¯æ–‡æ¡£..."):
            response = qa_agent(
                final_key,
                st.session_state["memory"],
                uploaded_file,
                question,
                ai_model
            )

        # æ˜¾ç¤ºç­”æ¡ˆ
        with st.chat_message("assistant"):
            st.markdown(f"**æŠ€æœ¯è§£ç­”**  \n{response['answer']}")
            if response.get("source_documents"):
                with st.expander("ğŸ“ å‚è€ƒæ¥æº"):
                    sources = list(
                        {doc.metadata.get("source", uploaded_file.name) for doc in response["source_documents"]})
                    st.markdown("**ç›¸å…³æ–‡æ¡£ï¼š**  \n" + "  \n".join(f"- {s}" for s in sources))

        # æ›´æ–°å†å²
        st.session_state["chat_history"] = response["chat_history"]

# å†å²å¯¹è¯ç®¡ç†
if "chat_history" in st.session_state:
    with st.expander("ğŸ“œ å†å²å¯¹è¯ï¼ˆç‚¹å‡»å±•å¼€ï¼‰", expanded=False):
        for msg in st.session_state["chat_history"]:
            if msg.type == "human":
                st.markdown(f"**ğŸ‘¤ ç”¨æˆ·**  \n{msg.content}")
            else:
                st.markdown(f"**ğŸ¤– AIåŠ©æ‰‹**  \n{msg.content}")
            st.divider()